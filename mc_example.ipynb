{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "from time import time, sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of workers is better to speed up Python tasks that do not need to communicate (not share memory) and are CPU intensive. However, combining the results from different tasks takes time. For Dask, there is no hard limit on scaling, but the task overhead will eventually start to swamp your calculation depending on how long each task takes to compute. \n",
    "\n",
    "The Dask scheduler has an overhead of around 200 microseconds per task, so if each task takes 1 second of work then the workers begin to swamp around 5000 workers, but if each task takes only 100ms then your scheduler can only saturate around 500 cores, and so on. Bear in mind that task duration imposes an inversely proportional constraint on scaling. So, if you want to scale even larger then your tasks will need to start doing more work in each task to avoid overhead. Often this involves moving inner for loops within tasks rather than spreading them out to many tasks.\n",
    "\n",
    "For threads, the standard python implementation uses a Global Interpreter Lock (GIL), which prevent two threads from executing simultaneously in the same program. However, libraries like Numpy bypass this limitation by running external code in C. So, in general more threads per worker are good for a program that spends most of its time in NumPy, SciPy, etc., and fewer threads per worker are better for simpler programs that spend most of their time in the Python interpreter. Furthermore, threads are best for IO tasks or tasks involving external systems because threads can combine their work more efficiently than processes (share memory space and efficiently read and write to the same variables).\n",
    "\n",
    "Triton (IBM POWER System AC922) has at least 16 cores per processor, so the rule of thumb for threads per Dask worker is to choose the square root of the number of cores per processor. For Triton for example, this would mean that one could assign 4 threads per worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=16, memory_limit='14GB', processes=True, threads_per_worker=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:46413</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>16</li>\n",
       "  <li><b>Cores: </b>64</li>\n",
       "  <li><b>Memory: </b>224.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:46413' processes=16 threads=64, memory=224.00 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo Estimate of $\\\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to estimate the number $\\\\pi$ using a Monte-Carlo method (https://en.wikipedia.org/wiki/Pi#Monte_Carlo_methods)\n",
    "exploiting that the area of a quarter circle of unit radius is $\\pi/4$ and that hence the probability of any randomly chosen point in a unit square to lie in a unit circle centered at a corner of the unit square is $\\pi/4$ as well.  So, for $N$ randomly chosen pairs $(x, y)$ with $x \\in [0, 1)$ and $y \\in [0, 1)$, we count the number $N_{circ}$ of pairs that also satisfy $(x^2 + y^2) < 1$ and estimate $\\pi \\approx 4 \\cdot N_{circ} / N$.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pi_mc(size_in_bytes, chunksize_in_bytes=200e6):\n",
    "    \"\"\"Calculate PI using a Monte Carlo estimate.\"\"\"\n",
    "    \n",
    "    size = int(size_in_bytes / 8)\n",
    "    chunksize = int(chunksize_in_bytes / 8)\n",
    "    \n",
    "    xy = da.random.uniform(0, 1,\n",
    "                           size=(size / 2, 2),\n",
    "                           chunks=(chunksize / 2, 2))\n",
    "    \n",
    "    in_circle = ((xy ** 2).sum(axis=-1) < 1)\n",
    "    pi = 4 * in_circle.mean()\n",
    "\n",
    "    return pi\n",
    "\n",
    "\n",
    "def print_pi_stats(size, pi, time_delta, num_workers):\n",
    "    \"\"\"Print pi, calculate offset from true value, and print stats.\"\"\"\n",
    "    print(f\"{size / 1e9} GB\\n\"\n",
    "          f\"\\tMC pi: {pi : 13.11f}\"\n",
    "          f\"\\tErr: {abs(pi - np.pi) : 10.3e}\\n\"\n",
    "          f\"\\tWorkers: {num_workers}\"\n",
    "          f\"\\t\\tTime single loop MC: {time_delta : 7.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunksize Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a good rule of thumb is to create arrays with a minimum chunksize of at least one million elements (e.g., a 1000x1000 matrix). If your chunks are too small, queueing up operations will be extremely slow, because Dask will translate each operation into a huge number of operations mapped across many chunks. \n",
    "\n",
    "Computation on Dask arrays with small chunks can also be slow, because each operation on a chunk has some fixed overhead from the Python interpreter and the Dask task executor. Morevover, with large arrays (10+ GB), the cost of queueing up Dask operations can be noticeable, and you may need to increase to larger chunksizes. So, chunk sizes between 10MB-1GB are common, depending on the availability of RAM (they need to fit) and the duration of computations. A chunk should be small enough to fit comfortably in memory as there will be many chunks in memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.26 s ± 290 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "10.0 GB\n",
      "\tMC pi:  3.14147844480\tErr:  1.142e-04\n",
      "\tWorkers: 16\t\tTime single loop MC:   4.451s\n",
      "38.9 s ± 574 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "100.0 GB\n",
      "\tMC pi:  3.14160869248\tErr:  1.604e-05\n",
      "\tWorkers: 16\t\tTime single loop MC:  47.545s\n"
     ]
    }
   ],
   "source": [
    "for size in (1e9 * n for n in (10, 100)):\n",
    "    %timeit pi = calc_pi_mc(size,chunksize_in_bytes=5e6).compute()\n",
    "    start = time()\n",
    "    pi = calc_pi_mc(size,chunksize_in_bytes=5e6).compute()\n",
    "    elaps = time() - start\n",
    "\n",
    "    print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticeable overhead and slow down with chuncksize way too small and as we increase the array size. Let's make a five fold increase in chunksize, and consider larger arrays of TBs,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.52 s ± 34.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "10.0 GB\n",
      "\tMC pi:  3.14162829440\tErr:  3.564e-05\n",
      "\tWorkers: 16\t\tTime single loop MC:   1.522s\n",
      "13 s ± 78.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "100.0 GB\n",
      "\tMC pi:  3.14159653312\tErr:  3.880e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:  15.399s\n",
      "1000.0 GB\n",
      "\tMC pi:  3.14158926131\tErr:  3.392e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:  137.760s\n",
      "2500.0 GB\n",
      "\tMC pi:  3.14159733786\tErr:  4.684e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:  342.625s\n"
     ]
    }
   ],
   "source": [
    "for size in (1e9 * n for n in (10, 100, 1000, 2500)):\n",
    "    if (size == 1e9 * 10) or (size == 1e9 * 100):\n",
    "        %timeit pi = calc_pi_mc(size,chunksize_in_bytes=25e6).compute()\n",
    "        start = time()\n",
    "        pi = calc_pi_mc(size,chunksize_in_bytes=25e6).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))\n",
    "    else:\n",
    "        start = time()\n",
    "        pi = calc_pi_mc(size,chunksize_in_bytes=25e6).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerable reduction in time for 10GB and 100GB. Let's increase it to about the optimal value for larger arrays in the next two estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.38 s ± 30.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "10.0 GB\n",
      "\tMC pi:  3.14159518080\tErr:  2.527e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:   1.330s\n",
      "10.6 s ± 43.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "100.0 GB\n",
      "\tMC pi:  3.14159720128\tErr:  4.548e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:  11.450s\n",
      "1000.0 GB\n",
      "\tMC pi:  3.14158536102\tErr:  7.293e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:  105.790s\n",
      "2500.0 GB\n",
      "\tMC pi:  3.14159773745\tErr:  5.084e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:  265.119s\n"
     ]
    }
   ],
   "source": [
    "for size in (1e9 * n for n in (10, 100, 1000, 2500)):\n",
    "    if (size == 1e9 * 10) or (size == 1e9 * 100):\n",
    "        %timeit pi = calc_pi_mc(size,chunksize_in_bytes=100e6).compute()\n",
    "        start = time()\n",
    "        pi = calc_pi_mc(size,chunksize_in_bytes=100e6).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))\n",
    "    else:\n",
    "        start = time()\n",
    "        pi = calc_pi_mc(size,chunksize_in_bytes=100e6).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.46 s ± 88.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "10.0 GB\n",
      "\tMC pi:  3.14153104640\tErr:  6.161e-05\n",
      "\tWorkers: 16\t\tTime single loop MC:   1.434s\n",
      "10.5 s ± 95.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "100.0 GB\n",
      "\tMC pi:  3.14160082560\tErr:  8.172e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:  10.652s\n",
      "1000.0 GB\n",
      "\tMC pi:  3.14160204998\tErr:  9.396e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:  100.579s\n",
      "2500.0 GB\n",
      "\tMC pi:  3.14158933402\tErr:  3.320e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:  249.243s\n"
     ]
    }
   ],
   "source": [
    "for size in (1e9 * n for n in (10, 100, 1000, 2500)):\n",
    "    if (size == 1e9 * 10) or (size == 1e9 * 100):\n",
    "        %timeit pi = calc_pi_mc(size,chunksize_in_bytes=200e6).compute()\n",
    "        start = time()\n",
    "        pi = calc_pi_mc(size,chunksize_in_bytes=200e6).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))\n",
    "    else:\n",
    "        start = time()\n",
    "        pi = calc_pi_mc(size,chunksize_in_bytes=200e6).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the last two estimates above there is some degradation of computing time for the smaller array and improvement for the larger (TB) ones. Let's overdue the chunksize in the next two estimates, but beware that we need increasingly more RAM to fit the larger chunks (from about minimum 4GB memory limit to 12GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.96 s ± 110 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "10.0 GB\n",
      "\tMC pi:  3.14166693120\tErr:  7.428e-05\n",
      "\tWorkers: 16\t\tTime single loop MC:   1.670s\n",
      "10.6 s ± 115 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "100.0 GB\n",
      "\tMC pi:  3.14160421056\tErr:  1.156e-05\n",
      "\tWorkers: 16\t\tTime single loop MC:  14.065s\n",
      "1000.0 GB\n",
      "\tMC pi:  3.14158625978\tErr:  6.394e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:  96.382s\n",
      "2500.0 GB\n",
      "\tMC pi:  3.14160002726\tErr:  7.374e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:  239.586s\n"
     ]
    }
   ],
   "source": [
    "for size in (1e9 * n for n in (10, 100, 1000, 2500)):\n",
    "    if (size == 1e9 * 10) or (size == 1e9 * 100):\n",
    "        %timeit pi = calc_pi_mc(size,chunksize_in_bytes=500e6).compute()\n",
    "        start = time()\n",
    "        pi = calc_pi_mc(size,chunksize_in_bytes=25e6).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))\n",
    "    else:\n",
    "        start = time()\n",
    "        pi = calc_pi_mc(size,chunksize_in_bytes=500e6).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.38 s ± 87.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "10.0 GB\n",
      "\tMC pi:  3.14147066880\tErr:  1.220e-04\n",
      "\tWorkers: 16\t\tTime single loop MC:   3.278s\n",
      "11.5 s ± 138 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "100.0 GB\n",
      "\tMC pi:  3.14157488384\tErr:  1.777e-05\n",
      "\tWorkers: 16\t\tTime single loop MC:  11.649s\n",
      "1000.0 GB\n",
      "\tMC pi:  3.14159792595\tErr:  5.272e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:  95.873s\n",
      "2500.0 GB\n",
      "\tMC pi:  3.14158702479\tErr:  5.629e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:  237.575s\n"
     ]
    }
   ],
   "source": [
    "for size in (1e9 * n for n in (10, 100, 1000, 2500)):\n",
    "    if (size == 1e9 * 10) or (size == 1e9 * 100):\n",
    "        %timeit pi = calc_pi_mc(size,chunksize_in_bytes=1000e6).compute()\n",
    "        start = time()\n",
    "        pi = calc_pi_mc(size,chunksize_in_bytes=1000e6).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))\n",
    "    else:\n",
    "        start = time()\n",
    "        pi = calc_pi_mc(size,chunksize_in_bytes=1000e6).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your chunks are too big, some of your computation may be wasted, because Dask only computes results one chunk at a time. Moreover, a chunk must be large enough so that computations on that chunk take significantly longer than the 1ms overhead per task that Dask scheduling incurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, what about automatic chunking? Automatic chunking expands or contracts all dimensions marked with \"auto\" to try to reach chunk sizes with a number of bytes equal to the config value array.chunk-size, which is set to 128MB by default, but which you can change in your configuration (YAML files in ~/.config/dask/ or /etc/dask/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To list your dask conf: \n",
    "#import dask\n",
    "#dask.config.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pi_mc_auto(size_in_bytes):\n",
    "    \"\"\"Calculate PI using a Monte Carlo estimate.\"\"\"\n",
    "    \n",
    "    size = int(size_in_bytes / 8)\n",
    "    xy = da.random.uniform(0, 1,\n",
    "                           size=(size / 2, 2),\n",
    "                           chunks=(\"auto\", 2))\n",
    "    \n",
    "    in_circle = ((xy ** 2).sum(axis=-1) < 1)\n",
    "    pi = 4 * in_circle.mean()\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 s ± 42.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "10.0 GB\n",
      "\tMC pi:  3.14171615360\tErr:  1.235e-04\n",
      "\tWorkers: 16\t\tTime single loop MC:   1.662s\n",
      "10.6 s ± 88.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "100.0 GB\n",
      "\tMC pi:  3.14160321216\tErr:  1.056e-05\n",
      "\tWorkers: 16\t\tTime single loop MC:  10.568s\n",
      "1000.0 GB\n",
      "\tMC pi:  3.14158107219\tErr:  1.158e-05\n",
      "\tWorkers: 16\t\tTime single loop MC:  102.963s\n",
      "2500.0 GB\n",
      "\tMC pi:  3.14159453222\tErr:  1.879e-06\n",
      "\tWorkers: 16\t\tTime single loop MC:  255.852s\n"
     ]
    }
   ],
   "source": [
    "for size in (1e9 * n for n in (10, 100, 1000, 2500)):\n",
    "    if (size == 1e9 * 10) or (size == 1e9 * 100):\n",
    "        %timeit pi = calc_pi_mc_auto(size).compute()\n",
    "        start = time()\n",
    "        pi = calc_pi_mc_auto(size).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))\n",
    "    else:\n",
    "        start = time()\n",
    "        pi = calc_pi_mc_auto(size).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about no chunk? Let's consider a small 1GB and 5GB arrays (spoiler, takes longer than the 100GB one),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pi_mc_nochunk(size_in_bytes):\n",
    "    \"\"\"Calculate PI using a Monte Carlo estimate.\"\"\"\n",
    "    \n",
    "    size = int(size_in_bytes / 8)\n",
    "    xy = da.random.uniform(0, 1,\n",
    "                           size=(size / 2, 2),\n",
    "                           chunks=(-1, 2))\n",
    "    \n",
    "    in_circle = ((xy ** 2).sum(axis=-1) < 1)\n",
    "    pi = 4 * in_circle.mean()\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11 s ± 46.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.0 GB\n",
      "\tMC pi:  3.14137587200\tErr:  2.168e-04\n",
      "\tWorkers: 16\t\tTime single loop MC:   3.095s\n",
      "15 s ± 114 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "5.0 GB\n",
      "\tMC pi:  3.14163559680\tErr:  4.294e-05\n",
      "\tWorkers: 16\t\tTime single loop MC:  14.984s\n"
     ]
    }
   ],
   "source": [
    "for size in (1e9 * n for n in (1, 5)):\n",
    "    %timeit pi = calc_pi_mc_nochunk(size).compute()\n",
    "    start = time()\n",
    "    pi = calc_pi_mc_nochunk(size).compute()\n",
    "    elaps = time() - start\n",
    "\n",
    "    print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll scale up the cluster to twice the initial workers and reduce the memory limit to half to keep the total RAM used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling from 16 to 32 workers.\n"
     ]
    }
   ],
   "source": [
    "new_num_workers = 2 * len(cluster.workers)\n",
    "\n",
    "print(f\"Scaling from {len(cluster.workers)} to {new_num_workers} workers.\")\n",
    "\n",
    "cluster.scale(new_num_workers)\n",
    "\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:46413</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>32</li>\n",
       "  <li><b>Cores: </b>128</li>\n",
       "  <li><b>Memory: </b>448.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:46413' processes=32 threads=128, memory=448.00 GB>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7 s ± 48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "10.0 GB\n",
      "\tMC pi:  3.14159820160\tErr:  5.548e-06\n",
      "\tWorkers: 32\t\tTime single loop MC:   1.610s\n",
      "9.03 s ± 66.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "100.0 GB\n",
      "\tMC pi:  3.14161061440\tErr:  1.796e-05\n",
      "\tWorkers: 32\t\tTime single loop MC:   9.292s\n",
      "1000.0 GB\n",
      "\tMC pi:  3.14160379200\tErr:  1.114e-05\n",
      "\tWorkers: 32\t\tTime single loop MC:  86.785s\n",
      "2500.0 GB\n",
      "\tMC pi:  3.14158614920\tErr:  6.504e-06\n",
      "\tWorkers: 32\t\tTime single loop MC:  218.076s\n"
     ]
    }
   ],
   "source": [
    "for size in (1e9 * n for n in (10, 100, 1000, 2500)):\n",
    "    if (size == 1e9 * 10) or (size == 1e9 * 100):\n",
    "        %timeit pi = calc_pi_mc(size).compute()\n",
    "        start = time()\n",
    "        pi = calc_pi_mc(size).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))\n",
    "    else:\n",
    "        start = time()\n",
    "        pi = calc_pi_mc(size).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the adaptive scaling feature which has the ability to add and remove workers as needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check docstring of distributed.Adaptive for keywords\n",
    "ca = cluster.adapt(\n",
    "    minimum=4, maximum=32);\n",
    "\n",
    "sleep(10)  # Allow for scale-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:46413</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>16</li>\n",
       "  <li><b>Memory: </b>56.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:46413' processes=4 threads=16, memory=56.00 GB>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.97 s ± 212 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "10.0 GB\n",
      "\tMC pi:  3.14163765760\tErr:  4.500e-05\n",
      "\tWorkers: 12\t\tTime single loop MC:   1.726s\n",
      "9.43 s ± 61.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "100.0 GB\n",
      "\tMC pi:  3.14159765504\tErr:  5.001e-06\n",
      "\tWorkers: 32\t\tTime single loop MC:  10.075s\n",
      "1000.0 GB\n",
      "\tMC pi:  3.14160448371\tErr:  1.183e-05\n",
      "\tWorkers: 32\t\tTime single loop MC:  90.389s\n",
      "2500.0 GB\n",
      "\tMC pi:  3.14159257869\tErr:  7.490e-08\n",
      "\tWorkers: 32\t\tTime single loop MC:  231.399s\n"
     ]
    }
   ],
   "source": [
    "for size in (1e9 * n for n in (10, 100, 1000, 2500)):\n",
    "    if (size == 1e9 * 10) or (size == 1e9 * 100):\n",
    "        %timeit pi = calc_pi_mc_auto(size).compute()\n",
    "        start = time()\n",
    "        pi = calc_pi_mc_auto(size).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))\n",
    "    else:\n",
    "        start = time()\n",
    "        pi = calc_pi_mc_auto(size).compute()\n",
    "        elaps = time() - start\n",
    "\n",
    "        print_pi_stats(size, pi, elaps,\n",
    "                   num_workers=len(cluster.workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we also used the auto-chunck. The adaptive scaling specifies which idle workers to release according to:\n",
    "\n",
    " - If there are unassigned tasks or any stealable tasks and no idle workers, or if the average memory use is over 50%, then increase the number of workers by a fixed factor (defaults to two).\n",
    "\n",
    " - If there are idle workers and the average memory use is below 50% then reclaim the idle workers with the least data on them (after moving data to nearby workers) until we’re near 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Chunksize Best Practices for Xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Chunks should align with the computation that you want to do. For example, if you plan to frequently slice along a particular dimension, then it’s more efficient if your chunks are aligned so that you have to touch fewer chunks. Moreover, if you want to add two arrays, then its convenient if those arrays have matching chunks patterns.\n",
    "\n",
    "- Do your spatial and temporal indexing (e.g., .sel() or .isel()) early in the pipeline, especially before calling resample() or groupby().\n",
    "\n",
    "- Save intermediate results to disk as a netCDF files (using to_netcdf()) and then load them again with open_dataset() for further computations. For example, if subtracting temporal mean from a dataset, save the temporal mean to disk before subtracting. Again, in theory, Dask should be able to do the computation in a streaming fashion, but in practice this is a fail case for the Dask scheduler, because it tries to keep every chunk of an array that it computes in memory.\n",
    "\n",
    "- Specify smaller chunks across space when using open_mfdataset() (e.g., chunks={'latitude': 10, 'longitude': 10}). This makes spatial subsetting easier, because there’s no risk you will load chunks of data referring to different chunks.\n",
    "\n",
    "- Using the h5netcdf package by passing engine='h5netcdf' to open_mfdataset() can be quicker than the default engine='netcdf4' that uses the netCDF4 package. Moreover, using the parallel = True argument in open_mfdataset() makes the open and preprocess steps to be performed in parallel using dask.delayed (Default is False).\n",
    "\n",
    "- The dask diagnostics can be useful in identifying performance bottlenecks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close workers and cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# This file may be used to create an environment using:\n",
      "# $ conda create --name <env> --file <this file>\n",
      "# platform: linux-ppc64le\n",
      "@EXPLICIT\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/_libgcc_mutex-0.1-conda_forge.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/ca-certificates-2020.4.5.1-hecc5488_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/ld_impl_linux-ppc64le-2.34-h0f24833_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libgfortran-ng-8.2.0-h822a55f_5.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libstdcxx-ng-8.2.0-h822a55f_5.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/llvm-openmp-10.0.0-h1bb5118_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/_openmp_mutex-4.5-1_llvm.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libgcc-ng-8.2.0-hdd5993f_5.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/expat-2.2.9-hb209c28_2.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/fribidi-1.0.9-h6eb9509_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/gmp-6.2.0-hb209c28_2.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/graphite2-1.3.13-hb209c28_1001.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/icu-64.2-hb209c28_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/jpeg-9c-h14c3975_1001.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libffi-3.2.1-hb209c28_1007.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libiconv-1.15-h6eb9509_1006.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libopenblas-0.3.7-ha38281c_7.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libsodium-1.0.17-h6eb9509_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libtool-2.4.6-h14c3975_1002.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libuuid-2.32.1-h14c3975_1000.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libuv-1.34.0-h6eb9509_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libwebp-base-1.1.0-h6eb9509_3.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/lz4-c-1.9.2-hb209c28_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/ncurses-6.1-hf484d3e_1002.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/openssl-1.1.1g-h6eb9509_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/pcre-8.44-hb209c28_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/pixman-0.38.0-h6eb9509_1003.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/pthread-stubs-0.4-h14c3975_1001.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/xorg-kbproto-1.0.7-h14c3975_1002.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/xorg-libice-1.0.10-h6eb9509_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/xorg-libxau-1.0.9-h14c3975_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/xorg-libxdmcp-1.1.3-h6eb9509_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/xorg-renderproto-0.11.1-h14c3975_1002.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/xorg-xextproto-7.3.0-h14c3975_1002.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/xorg-xproto-7.0.31-h14c3975_1007.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/xz-5.2.5-h6eb9509_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/yaml-0.2.4-h6eb9509_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/zlib-1.2.11-h6eb9509_1006.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/gettext-0.19.8.1-h94c31b8_1002.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libblas-3.8.0-14_openblas.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libpng-1.6.37-h151fe60_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libxcb-1.13-h14c3975_1002.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libxml2-2.9.10-hc938f6a_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/nodejs-13.13.0-hee65b04_0.tar.bz2\n",
      "https://repo.anaconda.com/pkgs/main/linux-ppc64le/pandoc-2.0.0.1-1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/readline-8.0-hf8c457e_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/tk-8.6.10-h151fe60_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/xorg-libsm-1.2.3-h83de47b_1000.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/zeromq-4.3.2-hb209c28_2.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/zstd-1.4.4-h1dc757f_3.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/freetype-2.10.1-h33812df_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libcblas-3.8.0-14_openblas.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/liblapack-3.8.0-14_openblas.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/libtiff-4.1.0-hee46ee4_6.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/sqlite-3.30.1-hd61ad8c_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/xorg-libx11-1.6.9-h6eb9509_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/fontconfig-2.13.1-h3f60eee_1001.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/python-3.7.6-haf90cd5_5_cpython.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/xorg-libxext-1.3.4-h6eb9509_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/xorg-libxrender-0.9.10-h6eb9509_1002.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/xorg-libxt-1.1.5-h6eb9509_1003.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/attrs-19.3.0-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/backcall-0.1.0-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/click-7.1.1-pyh8c360ce_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/cloudpickle-1.3.0-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/dask-core-2.15.0-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/decorator-4.4.2-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/defusedxml-0.6.0-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/glib-2.64.2-h1be4321_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/heapdict-1.0.1-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/idna-2.9-py_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/ipython_genutils-0.2.0-py_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/json5-0.9.0-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/pandocfilters-1.4.2-py_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/parso-0.7.0-pyh9f0ad1d_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/prometheus_client-0.7.1-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/ptyprocess-0.6.0-py_1001.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/pycparser-2.20-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/pyparsing-2.4.7-pyh9f0ad1d_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/python_abi-3.7-1_cp37m.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/send2trash-1.5.0-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/six-1.14.0-py_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/sortedcontainers-2.1.0-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/tblib-1.6.0-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/testpath-0.4.4-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/toolz-0.10.0-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/wcwidth-0.1.9-pyh9f0ad1d_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/webencodings-0.5.1-py_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/xorg-libxpm-3.5.13-h6eb9509_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/zipp-3.1.0-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/cairo-1.16.0-hd0b5358_1003.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/certifi-2020.4.5.1-py37hc8dfbb8_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/cffi-1.14.0-py37hc54e17a_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/chardet-3.0.4-py37hc8dfbb8_1006.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/cycler-0.10.0-py_2.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/cytoolz-0.10.1-py37h6eb9509_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/entrypoints-0.3-py37hc8dfbb8_1001.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/importlib-metadata-1.6.0-py37hc8dfbb8_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/jedi-0.17.0-py37hc8dfbb8_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/kiwisolver-1.2.0-py37h70ed317_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/markupsafe-1.1.1-py37h2bd1440_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/mistune-0.8.4-py37h2bd1440_1001.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/msgpack-python-1.0.0-py37h70ed317_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/numpy-1.18.1-py37h8edcdf1_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/pexpect-4.8.0-py37hc8dfbb8_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/pickleshare-0.7.5-py37hc8dfbb8_1001.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/psutil-5.7.0-py37h2bd1440_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/pyrsistent-0.16.0-py37h2bd1440_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/pysocks-1.7.1-py37hc8dfbb8_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.8.1-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/pyyaml-5.3.1-py37h2bd1440_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/pyzmq-19.0.0-py37h5fdba43_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/tornado-6.0.4-py37h2bd1440_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/traitlets-4.3.3-py37hc8dfbb8_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/zict-2.0.0-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/brotlipy-0.7.0-py37h2bd1440_1000.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/cryptography-2.8-py37hbd29ff1_2.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/harfbuzz-2.4.0-hc683052_3.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/importlib_metadata-1.6.0-0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/jupyter_core-4.6.3-py37hc8dfbb8_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/setuptools-46.1.3-py37hc8dfbb8_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/terminado-0.8.3-py37hc8dfbb8_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/bleach-3.1.4-pyh9f0ad1d_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/distributed-2.15.0-py37hc8dfbb8_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/jinja2-2.11.2-pyh9f0ad1d_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/jsonschema-3.2.0-py37hc8dfbb8_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/jupyter_client-6.1.3-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/matplotlib-base-3.2.1-py37h6d5b32b_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/pango-1.42.4-h8a55e4e_4.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/pygments-2.6.1-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/pyopenssl-19.1.0-py_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/wheel-0.34.2-py_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/dask-jobqueue-0.7.1-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/graphviz-2.42.3-h3586aca_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/matplotlib-3.2.1-0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/nbformat-5.0.6-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/pip-20.0.2-py_2.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/prompt-toolkit-3.0.5-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/urllib3-1.25.9-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/ipython-7.13.0-py37hc8dfbb8_2.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/nbconvert-5.6.1-py37hc8dfbb8_1.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/requests-2.23.0-pyh8c360ce_2.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/ipykernel-5.2.1-py37hc6149b9_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/notebook-6.0.3-py37_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/jupyterlab_server-1.1.1-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/nbserverproxy-0.8.8-py_1000.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/linux-ppc64le/widgetsnbextension-3.5.1-py37_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/ipywidgets-7.5.1-py_0.tar.bz2\n",
      "https://conda.anaconda.org/conda-forge/noarch/jupyterlab-2.0.0-py_1.tar.bz2\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda list --explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package              Version            \n",
      "-------------------- -------------------\n",
      "aiohttp              3.6.2              \n",
      "async-timeout        3.0.1              \n",
      "attrs                19.3.0             \n",
      "backcall             0.1.0              \n",
      "bleach               3.1.4              \n",
      "bokeh                1.4.0              \n",
      "brotlipy             0.7.0              \n",
      "certifi              2020.4.5.1         \n",
      "cffi                 1.14.0             \n",
      "chardet              3.0.4              \n",
      "click                7.1.1              \n",
      "cloudpickle          1.3.0              \n",
      "cryptography         2.8                \n",
      "cycler               0.10.0             \n",
      "cytoolz              0.10.1             \n",
      "dask                 2.15.0             \n",
      "dask-jobqueue        0.7.1              \n",
      "dask-labextension    2.0.1              \n",
      "decorator            4.4.2              \n",
      "defusedxml           0.6.0              \n",
      "distributed          2.15.0             \n",
      "entrypoints          0.3                \n",
      "HeapDict             1.0.1              \n",
      "idna                 2.9                \n",
      "importlib-metadata   1.6.0              \n",
      "ipykernel            5.2.1              \n",
      "ipython              7.13.0             \n",
      "ipython-genutils     0.2.0              \n",
      "ipywidgets           7.5.1              \n",
      "jedi                 0.17.0             \n",
      "Jinja2               2.11.2             \n",
      "json5                0.9.0              \n",
      "jsonschema           3.2.0              \n",
      "jupyter-client       6.1.3              \n",
      "jupyter-core         4.6.3              \n",
      "jupyter-server-proxy 1.3.2              \n",
      "jupyterlab           2.0.0              \n",
      "jupyterlab-server    1.1.1              \n",
      "kiwisolver           1.2.0              \n",
      "MarkupSafe           1.1.1              \n",
      "matplotlib           3.2.1              \n",
      "mistune              0.8.4              \n",
      "msgpack              1.0.0              \n",
      "multidict            4.7.5              \n",
      "nbconvert            5.6.1              \n",
      "nbformat             5.0.6              \n",
      "nbserverproxy        0.8.8              \n",
      "notebook             6.0.3              \n",
      "numpy                1.18.1             \n",
      "packaging            20.3               \n",
      "pandocfilters        1.4.2              \n",
      "parso                0.7.0              \n",
      "pexpect              4.8.0              \n",
      "pickleshare          0.7.5              \n",
      "Pillow               7.1.2              \n",
      "pip                  20.0.2             \n",
      "prometheus-client    0.7.1              \n",
      "prompt-toolkit       3.0.5              \n",
      "psutil               5.7.0              \n",
      "ptyprocess           0.6.0              \n",
      "pycparser            2.20               \n",
      "Pygments             2.6.1              \n",
      "pyOpenSSL            19.1.0             \n",
      "pyparsing            2.4.7              \n",
      "pyrsistent           0.16.0             \n",
      "PySocks              1.7.1              \n",
      "python-dateutil      2.8.1              \n",
      "PyYAML               5.3.1              \n",
      "pyzmq                19.0.0             \n",
      "requests             2.23.0             \n",
      "Send2Trash           1.5.0              \n",
      "setuptools           46.1.3.post20200325\n",
      "simpervisor          0.3                \n",
      "six                  1.14.0             \n",
      "sortedcontainers     2.1.0              \n",
      "tblib                1.6.0              \n",
      "terminado            0.8.3              \n",
      "testpath             0.4.4              \n",
      "toolz                0.10.0             \n",
      "tornado              6.0.4              \n",
      "traitlets            4.3.3              \n",
      "urllib3              1.25.9             \n",
      "wcwidth              0.1.9              \n",
      "webencodings         0.5.1              \n",
      "wheel                0.34.2             \n",
      "widgetsnbextension   3.5.1              \n",
      "yarl                 1.4.2              \n",
      "zict                 2.0.0              \n",
      "zipp                 3.1.0              \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
